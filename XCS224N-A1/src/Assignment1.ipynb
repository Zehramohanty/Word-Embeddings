{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256af773-9380-4842-bba9-4779ec95eca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/zehramohanty/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31067695-5eec-4f36-8ba3-34fb57c04e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    corpus_words = []\n",
    "    num_corpus_words = 0\n",
    "    \n",
    "    # ### START CODE HERE ###\n",
    "    templist = [i for j in corpus for i in j]\n",
    "    corpus_words = sorted(list(set(templist)))\n",
    "    num_corpus_words = len(corpus_words)\n",
    "        \n",
    "    # ### END CODE HERE ###\n",
    "\n",
    "    return corpus_words, num_corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2081bacd-77ec-47e4-b891-06f436fe3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "\n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "\n",
    "              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n",
    "              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)):\n",
    "                Co-occurrence matrix of word counts.\n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)  \n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "\n",
    "\n",
    "    # ### START CODE HERE ###\n",
    "    \n",
    "\n",
    "    M = np.zeros((num_words, num_words), dtype = int)\n",
    "    word2Ind = dict(zip(words, range(num_words)))\n",
    "    \n",
    "    for document in corpus: \n",
    "        length = len(document)\n",
    "        for counter in range(length): \n",
    "            ref_word = document[counter]\n",
    "            ref_word_index = word2Ind[ref_word]\n",
    "            for j in range(max(0, counter - window_size), min(length, counter + window_size + 1)):\n",
    "                target_word = document[j]\n",
    "                target_word_index = word2Ind[target_word]\n",
    "                if target_word != ref_word:\n",
    "                    M[target_word_index, ref_word_index] += 1\n",
    "                \n",
    "\n",
    "    # ### END CODE HERE ###\n",
    "\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e005a15-9b1f-48df-a5ed-591122725b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurrence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurrence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of unique words in the corpus, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"\n",
    "    np.random.seed(4355)\n",
    "    n_iter = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "\n",
    "    # ### START CODE HERE ###\n",
    "    trunc = TruncatedSVD(n_components=k, n_iter=n_iter)\n",
    "    M_reduced = trunc.fit_transform(M)\n",
    "    # ### END CODE HERE ###\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fd2e635-1fe4-4814-a336-c40551f8848c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     plot_embeddings(M_normalized, word2Ind_co_occurrence, words, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mco_occurrence_embeddings_(soln).png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#Read in the corpus\u001b[39;00m\n\u001b[1;32m     19\u001b[0m reuters_corpus \u001b[38;5;241m=\u001b[39m read_corpus()\n\u001b[0;32m---> 21\u001b[0m M_co_occurrence, word2Ind_co_occurrence \u001b[38;5;241m=\u001b[39m compute_co_occurrence_matrix(reuters_corpus)\n\u001b[1;32m     22\u001b[0m M_reduced_co_occurrence \u001b[38;5;241m=\u001b[39m reduce_to_k_dim(M_co_occurrence, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Rescale (normalize) the rows to make them each of unit-length\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    matplotlib.use('agg')\n",
    "    plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "    assert sys.version_info[0] == 3\n",
    "    assert sys.version_info[1] >= 5\n",
    "\n",
    "    def plot_embeddings(M_reduced, word2Ind, words, title):\n",
    "\n",
    "        for word in words:\n",
    "            idx = word2Ind[word]\n",
    "            x = M_reduced[idx, 0]\n",
    "            y = M_reduced[idx, 1]\n",
    "            plt.scatter(x, y, marker='x', color='red')\n",
    "            plt.text(x, y, word, fontsize=9)\n",
    "        plt.savefig(title)\n",
    "\n",
    "    #Read in the corpus\n",
    "    reuters_corpus = read_corpus()\n",
    "\n",
    "    M_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
    "    M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
    "    # Rescale (normalize) the rows to make them each of unit-length\n",
    "    M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
    "    M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
    "\n",
    "    words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
    "    plot_embeddings(M_normalized, word2Ind_co_occurrence, words, 'co_occurrence_embeddings_(soln).png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57205c-1fd5-4f08-849e-ab086b996ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbedb1d-9c44-45d2-b18c-a677a7c7c788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13c4be-44d4-473f-82e3-6e54df6b729b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XCS224N",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
